{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from code import *\n",
    "from useful_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{(k+1)}=Ax^{(k)}-\\alpha y^{(k)} $\n",
    "\n",
    "$V_{k+1}= AV_{k}$\n",
    "\n",
    "$y^{(k+1)}=Ay^{(k)}+D_{k+1}^{-1}(g^{(k+1)}-g^{(k)} ) $\n",
    "\n",
    "其中, $D_{k}=\\text{Diag}(V_k)^{-1} $\n",
    "\n",
    "计算最后的平均梯度用的是$\\mathbf{1}_n\\pi_A^T\\mathbf{x}^{(k)}$，不过和使用$\\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n^T\\mathbf{x}^{(k)}$“应该”区别不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_sum_original(A, init_x, h_data, y_data, grad_func, grad_f_bar_func, d=784, rho=0.1, lr=0.1,sigma_n=0.1, max_it=200, mg=1):\n",
    "    n = A.shape[0]\n",
    "    h, y = h_data, y_data\n",
    "    x = init_x\n",
    "    g = grad_func(x, y, h, rho=rho).reshape(x.shape)+sigma_n/mg*np.random.normal(size=(n,d))\n",
    "    v = g \n",
    "    pi=get_left_perron(A).reshape((1,n))\n",
    "    A0=np.ones((n, 1))@pi@A\n",
    "    B=get_B(A,9,n)\n",
    "    correction_vec=np.ones(n)\n",
    "    gradient_f_bar_x_val = grad_f_bar_func((A0@x).reshape(x.shape), y, h, rho=rho)\n",
    "    gradient_history=[np.linalg.norm(gradient_f_bar_x_val)]\n",
    "    v_history=[np.linalg.norm(v)]\n",
    "    for i in range(max_it):\n",
    "        \n",
    "        correction_vec=A.T@correction_vec \n",
    "        x = A @ x - lr * np.diag(1/correction_vec) @ v \n",
    "        pre_g = g\n",
    "        gradient=grad_func(x, y, h, rho=rho)\n",
    "        g = gradient.reshape(x.shape)+sigma_n/mg*np.random.normal(size=(n,d))\n",
    "        v = B @ v + g - pre_g\n",
    "        g1=grad_f_bar_func((A0@x).reshape(x.shape), y, h, rho=rho)\n",
    "        gradient_history.append(np.linalg.norm(g1))\n",
    "        v_history.append(np.linalg.norm(np.diag(1/correction_vec) @v))\n",
    "\n",
    "    return gradient_history ,v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_sum(\n",
    "    A,\n",
    "    init_x,\n",
    "    h_data,\n",
    "    y_data,\n",
    "    grad_func,\n",
    "    grad_f_bar_func,\n",
    "    d=784,\n",
    "    rho=0.1,\n",
    "    lr=0.1,\n",
    "    sigma_n=0.1,\n",
    "    max_it=200,\n",
    "    mg=1,\n",
    "    marm_up=10,\n",
    "):\n",
    "    \"\"\"新的 pull sum 算法\"\"\"\n",
    "    n = A.shape[0]\n",
    "    h, y = h_data, y_data\n",
    "    x = init_x\n",
    "    g = grad_func(x, y, h, rho=rho).reshape(x.shape) + sigma_n / mg * np.random.normal(\n",
    "        size=(n, d)\n",
    "    )\n",
    "    V = np.eye(n)\n",
    "    v = g\n",
    "\n",
    "    pi = get_left_perron(A).reshape((1, n))\n",
    "    A0 = np.ones((n, 1)) @ pi @ A\n",
    "    gradient_f_bar_x_val = grad_f_bar_func((A0 @ x).reshape(x.shape), y, h, rho=rho)\n",
    "    gradient_history = [np.linalg.norm(gradient_f_bar_x_val)]\n",
    "    v_history = [np.linalg.norm(v)]\n",
    "\n",
    "    for i in range(max_it):\n",
    "        if i < marm_up:\n",
    "            x = A @ x - lr / 100 * v\n",
    "        else:\n",
    "            x = A @ x - lr * v\n",
    "        pre_g = g\n",
    "        g = grad_func(x, y, h, rho=rho).reshape(\n",
    "            x.shape\n",
    "        ) + sigma_n / mg * np.random.normal(size=(n, d))\n",
    "        V = A @ V\n",
    "        D = np.diag(1 / np.diag(V))\n",
    "        v = A @ v + D @ (g - pre_g)\n",
    "\n",
    "        gradient_history.append(np.linalg.norm(g))\n",
    "        v_history.append(np.linalg.norm(v))\n",
    "    return gradient_history, v_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W=I$\n",
    "\n",
    "Ls 跟踪矩阵$A$的对角线元素的最小值的“倒数”，已知矩阵$A$的对角元会收敛到$\\pi_A$\n",
    "\n",
    "$x^{(k+1)}=Ax^{(k)}-lr*v^{(k)}$\n",
    "\n",
    "$v^{(k+1)}=Av^{(k)}+\\text{Diag}(A^{k+1})^{-1}g^{(k+1)}-\\text{Diag}(A^{k})^{-1}g^{(k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_diag(\n",
    "    A,\n",
    "    init_x,\n",
    "    h_data,\n",
    "    y_data,\n",
    "    grad_func,\n",
    "    grad_f_bar_func,\n",
    "    d=784,\n",
    "    rho=0.1,\n",
    "    lr=0.1,\n",
    "    sigma_n=0.1,\n",
    "    max_it=200,\n",
    "    mg=1,\n",
    "):\n",
    "    \"\"\"实际上是pull diag算法\"\"\"\n",
    "    n = A.shape[0]\n",
    "    h, y = h_data, y_data\n",
    "    x = init_x\n",
    "    W = np.eye(n)\n",
    "    g = grad_func(x, y, h, rho=rho).reshape(x.shape) + sigma_n / mg * np.random.normal(\n",
    "        size=(n, d)\n",
    "    )\n",
    "    w = np.linalg.inv(np.diag(np.diag(W))) @ g\n",
    "    v = g\n",
    "    pi = get_left_perron(A).reshape((1, n))\n",
    "    A0 = np.ones((n, 1)) @ pi @ A\n",
    "    gradient_f_bar_x_val = grad_f_bar_func((A0 @ x).reshape(x.shape), y, h, rho=rho)\n",
    "    gradient_history = [np.linalg.norm(gradient_f_bar_x_val)]\n",
    "    Ls = [1 / min(np.diag(A))]\n",
    "    v_history = [np.linalg.norm(v)]\n",
    "    for i in range(max_it):\n",
    "        W = A @ W\n",
    "        x = A @ x - lr * v\n",
    "        gradient = grad_func(x, y, h, rho=rho)\n",
    "        g = gradient.reshape(x.shape) + sigma_n / mg * np.random.normal(size=(n, d))\n",
    "        v = A @ v + np.linalg.inv(np.diag(np.diag(W))) @ g - w\n",
    "        w = (\n",
    "            np.linalg.inv(np.diag(np.diag(W))) @ g\n",
    "        )  # 这一步计算的w是下一步用到的w，因此程序没有问题\n",
    "        g1 = grad_f_bar_func((A0 @ x).reshape(x.shape), y, h, rho=rho)\n",
    "        gradient_history.append(np.linalg.norm(g1))\n",
    "        Ls.append(1 / min(np.diag(W)))\n",
    "        v_history.append(np.linalg.norm(v))\n",
    "\n",
    "    return gradient_history, v_history, max(Ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correction vector: $c^{(k)}=(A^T)^k\\mathbf{1}_n\\to n\\pi_A$\n",
    "\n",
    "$x^{(k+1)}= Ax^{(k)}-lr*\\text{Diag}(c^{(k)})^{-1}v^{(k)}$\n",
    "\n",
    "$v^{(k+1)}=\\textcolor{red}{A}v^{(k)}+g^{(k+1)}-g^{(k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_sum_AGT(A, init_x, h_data, y_data, grad_func, loss_func, grad_f_bar_func, d=784, L=1, rho=0.1, lr=0.1,sigma_n=0.1, max_it=200, mg=1, decay=1):\n",
    "    n = A.shape[0]\n",
    "    h, y = h_data, y_data\n",
    "    x = init_x\n",
    "    g = grad_func(x, y, h, rho=rho).reshape(x.shape)+sigma_n/mg*np.random.normal(size=(n,d))\n",
    "    v = g \n",
    "    pi=get_left_perron(A).reshape((1,n))\n",
    "    A0=np.ones((n, 1))@pi@A\n",
    "    \n",
    "    correction_vec=np.ones(n)\n",
    "    gradient_f_bar_x_val = grad_f_bar_func((A0@x).reshape(x.shape), y, h, rho=rho)\n",
    "    gradient_history=[np.linalg.norm(gradient_f_bar_x_val)]\n",
    "    v_history=[np.linalg.norm(v)]\n",
    "    for i in range(max_it):\n",
    "        \n",
    "        correction_vec=A.T@correction_vec \n",
    "        x = A @ x - lr * np.diag(1/correction_vec) @ v \n",
    "        pre_g = g\n",
    "        gradient=grad_func(x, y, h, rho=rho)\n",
    "        g = gradient.reshape(x.shape)+sigma_n/mg*np.random.normal(size=(n,d))\n",
    "        v = A @ v + g - pre_g\n",
    "        g1=grad_f_bar_func((A0@x).reshape(x.shape), y, h, rho=rho)\n",
    "        gradient_history.append(np.linalg.norm(g1))\n",
    "        v_history.append(np.linalg.norm(np.diag(1/correction_vec) @v))\n",
    "\n",
    "    return gradient_history ,v_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correction vector: $c^{(k)}=(A^T)^k\\mathbf{1}_n\\to n\\pi_A$\n",
    "\n",
    "$\\textcolor{blue}{B={get_B(A,9,n)}}$\n",
    "\n",
    "$x^{(k+1)}= Ax^{(k)}-lr*\\text{Diag}(c^{(k)})^{-1}v^{(k)}$\n",
    "\n",
    "$v^{(k+1)}=\\textcolor{blue}{B}v^{(k)}+g^{(k+1)}-g^{(k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_diag_BGT(A, init_x, h_data, y_data, grad_func, loss_func, grad_f_bar_func, d=784, L=1, rho=0.1, lr=0.1,sigma_n=0.1, max_it=200, mg=1, decay=1):\n",
    "    n = A.shape[0]\n",
    "    h, y = h_data, y_data\n",
    "    x = init_x\n",
    "    W = np.eye(n)\n",
    "    g = grad_func(x, y, h, rho=rho).reshape(x.shape)+sigma_n/mg*np.random.normal(size=(n,d))\n",
    "    w = np.linalg.inv(np.diag(np.diag(W)))@g\n",
    "    v= g\n",
    "    pi=get_left_perron(A).reshape((1,n))\n",
    "    A0=np.ones((n, 1))@pi@A\n",
    "    gradient_f_bar_x_val = grad_f_bar_func((A0@x).reshape(x.shape), y, h, rho=rho)\n",
    "    gradient_history=[np.linalg.norm(gradient_f_bar_x_val)]\n",
    "    Ls=[1/min(np.diag(A))]\n",
    "    v_history=[np.linalg.norm(v)]\n",
    "    B=get_B(A,9,n)\n",
    "    for i in range(max_it):\n",
    "        W=A@W\n",
    "        x = A @ x - lr *  v\n",
    "        gradient=grad_func(x, y, h, rho=rho)\n",
    "        g = gradient.reshape(x.shape)+sigma_n/mg*np.random.normal(size=(n,d))\n",
    "        v = B @ v + np.linalg.inv(np.diag(np.diag(W)))@g - w\n",
    "        w = np.linalg.inv(np.diag(np.diag(W)))@g\n",
    "        g1=grad_f_bar_func((A0@x).reshape(x.shape), y, h, rho=rho)\n",
    "        gradient_history.append(np.linalg.norm(g1))\n",
    "        Ls.append(1/min(np.diag(W)))\n",
    "        v_history.append(np.linalg.norm(v))\n",
    "\n",
    "    return gradient_history, v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kappa_row(A):\n",
    "    pi=get_left_perron(A)\n",
    "    return np.max(pi)/np.min(pi)\n",
    "\n",
    "def compute_kappa_col(B):\n",
    "    pi=get_right_perron(B)\n",
    "    return np.max(pi)/np.min(pi)\n",
    "\n",
    "#计算第二大特征值的模长\n",
    "def compute_2st_eig_value(A):\n",
    "    return abs(np.linalg.eigvals(A)[1])\n",
    "\n",
    "#计算beta\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from mpmath import mp\n",
    "\n",
    "def compute_beta_row(A, precision=64):\n",
    "    mp.dps = precision  # 设置计算精度\n",
    "    n = A.shape[0]\n",
    "    pi = get_left_perron(A)\n",
    "    one = np.ones(n)\n",
    "    if not nx.is_strongly_connected(nx.DiGraph(A)):\n",
    "        print(\"不是强联通\")\n",
    "    matrix = A - np.outer(one, pi)\n",
    "    diag1 = np.diag(np.sqrt(pi))\n",
    "    diag1_inverse = np.diag(1 / np.sqrt(pi))\n",
    "    result = np.linalg.norm(diag1 @ matrix @ diag1_inverse, 2)\n",
    "    return min(result, 1)  # 裁剪结果不超过1\n",
    "\n",
    "def compute_beta_col(B, precision=64):\n",
    "    mp.dps = precision  # 设置计算精度\n",
    "    n = B.shape[0]\n",
    "    pi = get_right_perron(B)\n",
    "    one = np.ones(n)\n",
    "    if not nx.is_strongly_connected(nx.DiGraph(B)):\n",
    "        print(\"不是强联通\")\n",
    "    matrix = B - np.outer(pi, one)\n",
    "    diag1 = np.diag(np.sqrt(pi))\n",
    "    diag1_inverse = np.diag(1 / np.sqrt(pi))\n",
    "    result = np.linalg.norm(diag1_inverse @ matrix @ diag1, 2)\n",
    "    return min(result, 1)  # 裁剪结果不超过1\n",
    "\n",
    "\n",
    "def compute_S_A_row(A):\n",
    "    kappa=compute_kappa_row(A)\n",
    "    beta=compute_beta_row(A)\n",
    "    n=A.shape[0]\n",
    "    output=2*np.sqrt(n)*(1+np.log(kappa))/(1-beta)\n",
    "    return output\n",
    "\n",
    "def compute_S_B_col(B):\n",
    "    kappa=compute_kappa_col(B)\n",
    "    beta=compute_beta_col(B)\n",
    "    n=B.shape[0]\n",
    "    output=2*np.sqrt(n)*(1+np.log(kappa))/(1-beta)\n",
    "    return output\n",
    "\n",
    "def show_row(A):\n",
    "    print(\"A的第二大特征值:\",compute_2st_eig_value(A))\n",
    "    print(\"A的beta:\",compute_beta_row(A))\n",
    "    print(\"A的spectral gap:\",1-compute_beta_row(A))\n",
    "    print(\"A的kappa:\",compute_kappa_row(A))\n",
    "    print(\"S_A是:\",compute_S_A_row(A),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
