{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from useful_functions import *\n",
    "from optimizer import *\n",
    "from model import *\n",
    "from MNIST_data_process import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from training import *\n",
    "from mlxtend.data import mnist_data\n",
    "from accuracy_compute import *\n",
    "from data_preparation_easy import *\n",
    "from data_preparation_hard import *\n",
    "from network_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一、 4层神经网络训练MNIST数据集：\n",
    "# 【1】异质性观察\n",
    "# 图1：pulldiag在di_ring(n=5)+三种不同异质性的表现(注：现在只弄了两种异质性，均匀分布和完全异质分布。能不能弄一个稍微混合一点的数据分布？比如在完全异质分布条件下，让1，2号节点数据混合一下。）\n",
    "# 图2：pullsum在di_ring(n=5)+三种不同异质性的表现\n",
    "# 图3：pulldiag和pullsum都在di_ring(n=5)或者di_ring(n=10)+强异质性下的对比表现\n",
    "\n",
    "# 【2】拓扑影响\n",
    "# 图1：在row_and_col_mat(n=10, p=0.5）+强异质性条件下比较pulldiag, pullsum, frsd, frozen \n",
    "# 图2：在row_and_col_mat(n=10, p=0.2）+强异质性条件下比较pulldiag, pullsum, frsd, frozen \n",
    "# 图3：只看pullsum, 在row_and_col_mat(n=10, p=0.5），row_and_col_mat(n=10, p=0.2），di_ring(n=10)，grid_10()上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图1：pulldiag在di_ring(n=5)+三种不同异质性的表现(注：现在只弄了两种异质性，均匀分布和完全异质分布。能不能弄一个稍微混合一点的数据分布？比如在完全异质分布条件下，让1，2号节点数据混合一下。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A的第二大特征值: 0.7360318786212668\n",
      "A的beta: 0.8102173609241419\n",
      "A的spectral gap: 0.18978263907585813\n",
      "A的kappa: 1.9999999999999976\n",
      "S_A是: 39.89819311270979 \n",
      "\n",
      "1.8413e+00, 2.7000e+01\n"
     ]
    }
   ],
   "source": [
    "#均匀分布数据\n",
    "n=5\n",
    "I=np.eye(n)\n",
    "one=np.ones(n)\n",
    "R=np.outer(one,one)/n\n",
    "A,C=di_ring(n=n)\n",
    "B=get_B(A=A,u=2*n,n=n)\n",
    "show_row(A)\n",
    "_,__=test_row(A)\n",
    "print(f\"{_:.4e}, {__:.4e}\")\n",
    "h_data,y_data,X_test,y_test=prepare_node_5_hard_shuffled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-09-09 15:44:47,562] A new study created in memory with name: no-name-10e827e8-ca72-4d4b-82e7-c367ee073aee\n",
      "C:\\Users\\16017\\AppData\\Local\\Temp\\ipykernel_2676\\3460508082.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr',1e-3 , 0.5)\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.09it/s, accuracy=80.1400000000%, epoch=25, loss=1.3028164387]\n",
      "[I 2024-09-09 15:45:10,650] Trial 0 finished with value: 0.8014 and parameters: {'lr': 0.32985083140176713}. Best is trial 0 with value: 0.8014.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.09it/s, accuracy=89.9300000000%, epoch=25, loss=0.6618429542]\n",
      "[I 2024-09-09 15:45:33,771] Trial 1 finished with value: 0.8993 and parameters: {'lr': 0.05636889806129744}. Best is trial 1 with value: 0.8993.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.09it/s, accuracy=88.2100000000%, epoch=25, loss=0.8402729273]\n",
      "[I 2024-09-09 15:45:56,858] Trial 2 finished with value: 0.8821 and parameters: {'lr': 0.10378260215911579}. Best is trial 1 with value: 0.8993.\n",
      "Training Progress: 100%|██████████| 25/25 [00:23<00:00,  1.07it/s, accuracy=85.5200000000%, epoch=25, loss=1.0286107063]\n",
      "[I 2024-09-09 15:46:20,424] Trial 3 finished with value: 0.8552 and parameters: {'lr': 0.006401111065044866}. Best is trial 1 with value: 0.8993.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.10it/s, accuracy=88.6100000000%, epoch=25, loss=0.6587769508]\n",
      "[I 2024-09-09 15:46:43,402] Trial 4 finished with value: 0.8861 and parameters: {'lr': 0.11375346293657185}. Best is trial 1 with value: 0.8993.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.11it/s, accuracy=87.8800000000%, epoch=25, loss=0.8489961505]\n",
      "[I 2024-09-09 15:47:06,103] Trial 5 finished with value: 0.8788 and parameters: {'lr': 0.10279353136032718}. Best is trial 1 with value: 0.8993.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.10it/s, accuracy=87.3800000000%, epoch=25, loss=0.6537586808]\n",
      "[I 2024-09-09 15:47:28,923] Trial 6 finished with value: 0.8738 and parameters: {'lr': 0.1699847478858443}. Best is trial 1 with value: 0.8993.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.09it/s, accuracy=89.5300000000%, epoch=25, loss=0.6831554890]\n",
      "[I 2024-09-09 15:47:52,057] Trial 7 finished with value: 0.8953 and parameters: {'lr': 0.02959880392855741}. Best is trial 1 with value: 0.8993.\n",
      "Training Progress: 100%|██████████| 25/25 [00:23<00:00,  1.08it/s, accuracy=90.0100000000%, epoch=25, loss=0.7017591596]\n",
      "[I 2024-09-09 15:48:15,335] Trial 8 finished with value: 0.9001 and parameters: {'lr': 0.04037018731902673}. Best is trial 8 with value: 0.9001.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.09it/s, accuracy=89.5100000000%, epoch=25, loss=0.6574491143]\n",
      "[I 2024-09-09 15:48:38,405] Trial 9 finished with value: 0.8951 and parameters: {'lr': 0.021503928251895645}. Best is trial 8 with value: 0.9001.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.11it/s, accuracy=74.9400000000%, epoch=25, loss=1.7604754448]\n",
      "[I 2024-09-09 15:49:01,126] Trial 10 finished with value: 0.7494 and parameters: {'lr': 0.001154843318135722}. Best is trial 8 with value: 0.9001.\n",
      "Training Progress: 100%|██████████| 25/25 [00:27<00:00,  1.11s/it, accuracy=89.6500000000%, epoch=25, loss=0.6512770653]\n",
      "[I 2024-09-09 15:49:29,001] Trial 11 finished with value: 0.8965 and parameters: {'lr': 0.023956666423008203}. Best is trial 8 with value: 0.9001.\n",
      "Training Progress: 100%|██████████| 25/25 [00:27<00:00,  1.12s/it, accuracy=87.5500000000%, epoch=25, loss=0.8371620536]\n",
      "[I 2024-09-09 15:49:57,085] Trial 12 finished with value: 0.8755 and parameters: {'lr': 0.01007497292176682}. Best is trial 8 with value: 0.9001.\n",
      "Training Progress: 100%|██████████| 25/25 [00:35<00:00,  1.41s/it, accuracy=90.1300000000%, epoch=25, loss=0.6560736179]\n",
      "[I 2024-09-09 15:50:32,644] Trial 13 finished with value: 0.9013 and parameters: {'lr': 0.046833244265188444}. Best is trial 13 with value: 0.9013.\n",
      "Training Progress: 100%|██████████| 25/25 [00:47<00:00,  1.90s/it, accuracy=84.5500000000%, epoch=25, loss=1.1020817518]\n",
      "[I 2024-09-09 15:51:20,670] Trial 14 finished with value: 0.8455 and parameters: {'lr': 0.005442128232046449}. Best is trial 13 with value: 0.9013.\n",
      "Training Progress: 100%|██████████| 25/25 [00:36<00:00,  1.47s/it, accuracy=52.5500000000%, epoch=25, loss=1.7158456087]\n",
      "[I 2024-09-09 15:51:57,727] Trial 15 finished with value: 0.5255 and parameters: {'lr': 0.45964453921409243}. Best is trial 13 with value: 0.9013.\n",
      "Training Progress: 100%|██████████| 25/25 [00:22<00:00,  1.12it/s, accuracy=90.0100000000%, epoch=25, loss=0.6493273020]\n",
      "[I 2024-09-09 15:52:20,332] Trial 16 finished with value: 0.9001 and parameters: {'lr': 0.0526504941468204}. Best is trial 13 with value: 0.9013.\n",
      "Training Progress: 100%|██████████| 25/25 [00:33<00:00,  1.34s/it, accuracy=88.3300000000%, epoch=25, loss=0.7566236615]\n",
      "[I 2024-09-09 15:52:54,117] Trial 17 finished with value: 0.8833 and parameters: {'lr': 0.012563743902141765}. Best is trial 13 with value: 0.9013.\n",
      "Training Progress: 100%|██████████| 25/25 [00:47<00:00,  1.92s/it, accuracy=79.6600000000%, epoch=25, loss=1.5398377419]\n",
      "[I 2024-09-09 15:53:42,562] Trial 18 finished with value: 0.7966 and parameters: {'lr': 0.002044150537899517}. Best is trial 13 with value: 0.9013.\n",
      "Training Progress: 100%|██████████| 25/25 [00:34<00:00,  1.37s/it, accuracy=89.9000000000%, epoch=25, loss=0.6641148567]\n",
      "[I 2024-09-09 15:54:17,289] Trial 19 finished with value: 0.899 and parameters: {'lr': 0.05728602003640276}. Best is trial 13 with value: 0.9013.\n",
      "Training Progress:  64%|██████▍   | 16/25 [00:14<00:08,  1.10it/s, accuracy=71.3500000000%, epoch=16, loss=1.0769938588]\n",
      "[W 2024-09-09 15:54:32,040] Trial 20 failed with parameters: {'lr': 0.19557015644173759} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\16017\\AppData\\Local\\Temp\\ipykernel_2676\\3460508082.py\", line 9, in objective\n",
      "    Diag_l1, Diag_a1 = train_PullDiag(\n",
      "  File \"d:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\training.py\", line 166, in train_PullDiag\n",
      "    loss = optimizer.step(closure)  # 调用优化器的 step 方法，并传入 closure\n",
      "  File \"d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"d:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\optimizer.py\", line 160, in step\n",
      "    loss = closure()  # 自动完成了反向传播\n",
      "  File \"d:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\training.py\", line 151, in closure\n",
      "    output = model(h_data[i])\n",
      "  File \"d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\model.py\", line 37, in forward\n",
      "    x = F.relu(self.bn1(self.fc1(x)))       # 第一层，先全连接，再批归一化，最后 ReLU 激活\n",
      "  File \"d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2024-09-09 15:54:32,043] Trial 20 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 创建一个优化器并使用缩小后的搜索空间进行优化\u001b[39;00m\n\u001b[0;32m     38\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      6\u001b[0m lr \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_loguniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m1e-3\u001b[39m , \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     Diag_l1, Diag_a1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_PullDiag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMNISTClassifier_2layer_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed_for_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m49\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_accuracy_with_average_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# 检查 FROZEN_l1 是否包含 inf 或 nan\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(Diag_l1)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(Diag_l1)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32md:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\training.py:166\u001b[0m, in \u001b[0;36mtrain_PullDiag\u001b[1;34m(n, A, model_class, seed_for_model, criterion_class, epochs, lr, X_train_data, y_train_data, X_test_data, y_test_data, compute_accuracy, show_graph)\u001b[0m\n\u001b[0;32m    163\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m--> 166\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 调用优化器的 step 方法，并传入 closure\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m    168\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m compute_accuracy(model_list, X_test_tensor, y_test_tensor)  \u001b[38;5;66;03m# 计算测试集上的准确率\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\optimizer.py:160\u001b[0m, in \u001b[0;36mPullDiag.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_list:\n\u001b[0;32m    159\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 160\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 自动完成了反向传播\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# step4: v = Av + W @ g - prev_W @ prev_g\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     new_v_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\training.py:151\u001b[0m, in \u001b[0;36mtrain_PullDiag.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    150\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 151\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y_data[i])\n\u001b[0;32m    153\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Desktop_copy\\大三上\\数值实验，pull diag\\远程仓库\\code\\神经网络实验\\model.py:37\u001b[0m, in \u001b[0;36mMNISTClassifier_2layer_2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 37\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))       \u001b[38;5;66;03m# 第一层，先全连接，再批归一化，最后 ReLU 激活\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)                     \u001b[38;5;66;03m# Dropout 层，随机丢弃部分神经元\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)                         \u001b[38;5;66;03m# 第二层，全连接输出层（不使用激活函数）\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    # 缩小 lr 的搜索范围，在已找到的最佳值附近搜索\n",
    "    lr = trial.suggest_loguniform('lr',1e-3 , 0.5)\n",
    "\n",
    "    try:\n",
    "        Diag_l1, Diag_a1 = train_PullDiag(\n",
    "            n=n,\n",
    "            A=A,\n",
    "            model_class=MNISTClassifier_2layer_2,\n",
    "            seed_for_model=49,\n",
    "            criterion_class=nn.CrossEntropyLoss,\n",
    "            epochs=25,\n",
    "            lr=lr,\n",
    "            X_train_data=h_data,\n",
    "            y_train_data=y_data,\n",
    "            X_test_data=X_test,\n",
    "            y_test_data=y_test,\n",
    "            compute_accuracy=compute_accuracy_with_average_model,\n",
    "            show_graph=False\n",
    "        )\n",
    "\n",
    "        # 检查 FROZEN_l1 是否包含 inf 或 nan\n",
    "        if np.isnan(Diag_l1).any() or np.isinf(Diag_l1).any():\n",
    "            print(f\"Trial failed due to inf/nan in loss. lr: {lr}\")\n",
    "            return -np.inf  # 返回一个非常低的值\n",
    "\n",
    "        # 返回最终的准确率\n",
    "        return Diag_a1[-1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed with exception: {e}\")\n",
    "        return -np.inf  # 若发生异常，返回一个非常低的值\n",
    "\n",
    "# 创建一个优化器并使用缩小后的搜索空间进行优化\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best accuracy: {study.best_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
