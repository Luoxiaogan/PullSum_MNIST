{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from resnet import ResNet18\n",
    "from network_func import *\n",
    "from optimizer import *\n",
    "from training import *\n",
    "from useful_functions import *\n",
    "from CIFAR10_data_preparation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A的第二大特征值: 0.7360318786212677\n",
      "A的beta: 0.810217360924142\n",
      "A的spectral gap: 0.18978263907585802\n",
      "A的kappa: 2.000000000000005\n",
      "S_A是: 39.898193112709905 \n",
      "\n",
      "1.8413e+00, 2.7000e+01\n"
     ]
    }
   ],
   "source": [
    "n=5\n",
    "I=np.eye(n)\n",
    "one=np.ones(n)\n",
    "R=np.outer(one,one)/n\n",
    "A,C=di_ring(n=n)\n",
    "B=get_B(A=A,u=2*n,n=n)\n",
    "show_row(A)\n",
    "_,__=test_row(A)\n",
    "print(f\"{_:.4e}, {__:.4e}\")\n",
    "h_data,y_data,X_test,y_test=cifar10_prepare_node_5_hard_shuffled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_with_weighted_model_resnet18(model_list, X_test, y_test, weights, criterion, testloader):\n",
    "    # 确保 X_test 和 y_test 在相同的设备上\n",
    "    device = next(model_list[0].parameters()).device\n",
    "\n",
    "    # Step 1: 计算加权模型的参数\n",
    "    avg_model = ResNet18(num_classes=10).to(device)\n",
    "    avg_state_dict = avg_model.state_dict()\n",
    "\n",
    "    # 初始化加权的参数字典\n",
    "    weighted_state_dict = {key: torch.zeros_like(param).to(device) for key, param in avg_state_dict.items()}\n",
    "\n",
    "    # 汇总加权参数\n",
    "    for model, weight in zip(model_list, weights):\n",
    "        state_dict = model.state_dict()\n",
    "        for key in weighted_state_dict.keys():\n",
    "            weighted_state_dict[key] += state_dict[key].to(device) * weight\n",
    "\n",
    "    # 计算总权重\n",
    "    total_weight = sum(weights)\n",
    "    weighted_avg_state_dict = {key: value / total_weight for key, value in weighted_state_dict.items()}\n",
    "\n",
    "    # 将加权参数加载到新模型中\n",
    "    avg_model.load_state_dict(weighted_avg_state_dict)\n",
    "\n",
    "    # 确保测试数据在正确的设备上\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "    # Step 2: 评估加权模型的准确性\n",
    "    avg_model.eval()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = avg_model(X_test)\n",
    "        loss = criterion(outputs, y_test)\n",
    "        sum_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_test.size(0)\n",
    "        correct += predicted.eq(y_test.data).cpu().sum()\n",
    "\n",
    "    val_loss = sum_loss / len(testloader)\n",
    "    val_accu = 100. * correct / total\n",
    "    \n",
    "    return val_loss, val_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import optimizer\n",
    "importlib.reload(optimizer)\n",
    "from optimizer import *\n",
    "\n",
    "import training\n",
    "importlib.reload(training)\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer初始化成功!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/5 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Sum_l1\u001b[38;5;241m=\u001b[39m \u001b[43msix_GPU_train_PullSum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResNet18\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed_for_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m49\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_accuracy_with_weighted_model_resnet18\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GanLuo/PullSum_MNIST/code/神经网络实验/CIFAR10最终实验/training.py:146\u001b[0m, in \u001b[0;36msix_GPU_train_PullSum\u001b[0;34m(n, A, B, model_class, seed_for_model, criterion_class, epochs, lr, X_train_data, y_train_data, X_test_data, y_test_data, compute_accuracy, batch_size, show_graph)\u001b[0m\n\u001b[1;32m    144\u001b[0m         batch_h_list\u001b[38;5;241m.\u001b[39mappend(batch_h)\n\u001b[1;32m    145\u001b[0m         batch_y_list\u001b[38;5;241m.\u001b[39mappend(batch_y)\n\u001b[0;32m--> 146\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_h_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    148\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loaders[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/GanLuo/PullSum_MNIST/code/神经网络实验/CIFAR10最终实验/optimizer.py:84\u001b[0m, in \u001b[0;36mPullSum_for_try.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_list:\n\u001b[1;32m     83\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 84\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 自动完成了反向传播\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# step4: v = Bv + g - prev_g\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     new_v_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/GanLuo/PullSum_MNIST/code/神经网络实验/CIFAR10最终实验/training.py:146\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m     loss = optimizer.step(closure)\n\u001b[1;32m    145\u001b[0m     epoch_loss = loss\n\u001b[0;32m--> 146\u001b[0m else:\n\u001b[1;32m    147\u001b[0m     for batch_tuple in zip(*train_loaders):\n\u001b[1;32m    148\u001b[0m         batch_h_list = []\n",
      "File \u001b[0;32m~/GanLuo/PullSum_MNIST/code/神经网络实验/CIFAR10最终实验/training.py:125\u001b[0m, in \u001b[0;36mclosure_for_batch\u001b[0;34m(batch_h_list, batch_y_list)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#后面的batch中不需要更改optimizer, 只需要更改h_data_train和y_data_train的定义就行了\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer初始化成功!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m train_loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure_for_batch\u001b[39m(batch_h_list, batch_y_list):\n\u001b[1;32m    128\u001b[0m     loss_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Sum_l1= six_GPU_train_PullSum(\n",
    "    n=n,\n",
    "    A=A,\n",
    "    B=B,\n",
    "    model_class=ResNet18,\n",
    "    seed_for_model=49,\n",
    "    criterion_class=nn.CrossEntropyLoss,\n",
    "    epochs=5,\n",
    "    lr=5e-3,\n",
    "    batch_size=128,\n",
    "    X_train_data=h_data,\n",
    "    y_train_data=y_data,\n",
    "    X_test_data=X_test,\n",
    "    y_test_data=y_test,\n",
    "    compute_accuracy=compute_accuracy_with_weighted_model_resnet18,\n",
    "    show_graph=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
